{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn.datasets import fetch_openml\r\n",
    "import joblib\r\n",
    "\r\n",
    "# Uncomment to download the dataset for the first time (takes about 30 seconds)\r\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\r\n",
    "joblib.dump(mnist, \"mnist.pkl\")\r\n",
    "\r\n",
    "# mnist = joblib.load(\"mnist.pkl\")\r\n",
    "mnist.keys()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\r\n",
    "X.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib as mpl\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "some_digit = X[0]\r\n",
    "some_digit_image = some_digit.reshape(28, 28)\r\n",
    "\r\n",
    "plt.imshow(some_digit_image, cmap=\"binary\")\r\n",
    "plt.axis(\"off\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "y = y.astype(np.uint8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train_5 = (y_train == 5)\r\n",
    "y_test_5 = (y_test == 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import SGDClassifier\r\n",
    "\r\n",
    "sgd_clf = SGDClassifier(random_state=42)\r\n",
    "sgd_clf.fit(X_train, y_train_5)\r\n",
    "joblib.dump(sgd_clf, \"sgd_clf.pkl\")\r\n",
    "# Uncomment if you want to load saved classifier from previous runs instead of training again\r\n",
    "# sgd_clf = joblib.load(\"sgd_clf.pkl\")\r\n",
    "\r\n",
    "sgd_clf.predict([some_digit])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Custom implementation of Cross-Validation\r\n",
    "\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "from sklearn.base import clone\r\n",
    "\r\n",
    "skfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\r\n",
    "\r\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\r\n",
    "    clone_clf = clone(sgd_clf)\r\n",
    "    X_train_folds = X_train[train_index]\r\n",
    "    y_train_folds = y_train_5[train_index]\r\n",
    "    X_test_fold = X_train[test_index]\r\n",
    "    y_test_fold = y_train_5[test_index]\r\n",
    "\r\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\r\n",
    "    y_pred = clone_clf.predict(X_test_fold)\r\n",
    "    n_correct = sum(y_pred == y_test_fold)\r\n",
    "    print(n_correct / len(y_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_score\r\n",
    "\r\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.base import BaseEstimator\r\n",
    "\r\n",
    "class Never5Classifier(BaseEstimator):\r\n",
    "    def fit(self, X, y=None):\r\n",
    "        pass\r\n",
    "    def predict(self, X):\r\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "never_5_clf = Never5Classifier()\r\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_predict\r\n",
    "\r\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\r\n",
    "\r\n",
    "confusion_matrix(y_train_5, y_train_pred)\r\n",
    "\r\n",
    "# output:\r\n",
    "#                            Predicted    Predicted\r\n",
    "#                         as non-fives:   as fives:        \r\n",
    "# Actual images of non-fives:  [[53892,   687],\r\n",
    "# Actual images of fives:       [ 1891,  3530]]\r\n",
    "#\r\n",
    "#                               [[TN,  FP]\r\n",
    "#                                [FN,  TP]]\r\n",
    "#\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train_perfect_predictions = y_train_5 # pretend we reached perfection\r\n",
    "confusion_matrix(y_train_5, y_train_perfect_predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import precision_score, recall_score\r\n",
    "precision_score(y_train_5, y_train_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "recall_score(y_train_5, y_train_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# F1 is the \"harmonic mean\". The regular mean gives equal weight to all values, but the harmonic mean gives more\r\n",
    "#  weight to low values. You have to have a high values for both precision and recall to have a good F1 score, or\r\n",
    "# more specifically, the F1 score favors classifiers that have a similar precision and recall.\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "f1_score(y_train_5, y_train_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_scores = sgd_clf.decision_function([some_digit])\r\n",
    "y_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "threshold = 0\r\n",
    "y_some_digit_pred = (y_scores > threshold)\r\n",
    "y_some_digit_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "threshold = 8000\r\n",
    "y_some_digit_pred = (y_scores > threshold)\r\n",
    "y_some_digit_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import precision_recall_curve\r\n",
    "\r\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\r\n",
    "\r\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\r\n",
    "    plt.figure(figsize=(10,7))    \r\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\r\n",
    "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\r\n",
    "    plt.legend()\r\n",
    "    plt.title(\"P-R Curve\")\r\n",
    "    plt.xlabel(\"Threshold\")\r\n",
    "    plt.grid()\r\n",
    "\r\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\r\n",
    "plt.axis([-30000, 30000, 0, 1])\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Obtaining the threshhold for a given precision or recall\r\n",
    "\r\n",
    "# What's happening here is a little obscure. It is based on the fact that precision_recall_curve() returned all the\r\n",
    "# possible precision values in ascending order. Since we are calling argmax on an array of bools there is now \"max\" and\r\n",
    "# it just returns the index of the first True it finds. Thus we use argmax to find the exact threshhold that gives us \r\n",
    "# 90% precision.\r\n",
    "\r\n",
    "plt.plot(precisions)\r\n",
    "\r\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\r\n",
    "print(threshold_90_precision)\r\n",
    "y_train_pred_90 = (y_scores >= threshold_90_precision)\r\n",
    "print(precision_score(y_train_5, y_train_pred_90))\r\n",
    "print(recall_score(y_train_5, y_train_pred_90))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_auc_score\r\n",
    "roc_auc_score(y_train_5, y_scores)\r\n",
    "\r\n",
    "# AUC (Area Under the Curve) should range from 0.5 (the random classifier) to 1.0 (perfect)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_curve\r\n",
    "\r\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\r\n",
    "\r\n",
    "def plot_roc_curve(fpr, tpr, label=None):\r\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\r\n",
    "    # A straight line with a slope of one represents a random classifier operating on coin\r\n",
    "    # flips. The ROC AUC of that classifier will always be 0.5, so that is the worst\r\n",
    "    # possible score and we want to be as far above that as possible.\r\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\r\n",
    "    plt.title(\"ROC AUC\")\r\n",
    "    plt.xlabel(\"False Positive Rate\")\r\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\")\r\n",
    "\r\n",
    "plot_roc_curve(fpr, tpr)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to choose between ROC curve and Precision-Recall curve\r\n",
    "\r\n",
    "In most cases, we prefer to evaluate based on the ROC curve. However, if you care a lot about false positives (which are highlighted by Precision), or as in our case the postive class is rare, the P-R curve can illustrate this more dramatically. For example, we have a very high ROC AUC, but this is only because 5's occur just 10% of the time. We know from our P-R curve that Precision and Recall are only about 73% when they are equal and tradeoff is fairly steep after that, so there is actually a lot of room for improvement with this classifier.\r\n",
    "\r\n",
    "Note: F1 is the preferred metric for summarizing the P-R curve."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compare now with Random Forest Classifier\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "forest_clf = RandomForestClassifier(random_state=42)\r\n",
    "joblib.dump(forest_clf, \"forest_clf.pkl\")\r\n",
    "# Uncomment if you want to load saved classifier from previous runs instead of training again\r\n",
    "# forest_clf = joblib.load(\"forest_clf.pkl\")\r\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method=\"predict_proba\")\r\n",
    "\r\n",
    "# Random Forest's predict_proba method returns an n x c matrix, where it has a column showing the probability for each class\r\n",
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\r\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)\r\n",
    "\r\n",
    "# Curve 1 on the plot\r\n",
    "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\r\n",
    "# Curve 2 on the plot\r\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\r\n",
    "plt.legend(loc=\"lower right\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\r\n",
    "\r\n",
    "sgd_auc = roc_auc_score(y_train_5, y_scores)\r\n",
    "forest_auc = roc_auc_score(y_train_5, y_scores_forest)\r\n",
    "sgd_f1 = f1_score(y_train_5, y_train_pred)\r\n",
    "forest_f1 = f1_score(y_train_5, y_train_pred_forest)\r\n",
    "\r\n",
    "print(f\"SGD AUC: {sgd_auc}\")\r\n",
    "print(f\"Forest AUC: {forest_auc}\")\r\n",
    "print(f\"SGD F1: {sgd_f1}\")\r\n",
    "print(f\"Forest F1: {forest_f1}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "precisions_f, recalls_f, thresholds_f = precision_recall_curve(y_train_5, y_scores_forest)\r\n",
    "\r\n",
    "plot_precision_recall_vs_threshold(precisions_f, recalls_f, thresholds_f)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(y_scores_forest))\r\n",
    "print(len(thresholds_f))\r\n",
    "\r\n",
    "# There's an interesting quirk happening here. We're supposed to get a threshold for each\r\n",
    "# instance in our training set, but in this case we get very few. There is a small note\r\n",
    "# in the docs that mentions that we only get a number of thresholds equal to the unique\r\n",
    "# scores given to precision_recall_curve(): https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\r\n",
    "\r\n",
    "# This prevented me from setting a high-recall threshold for random_forest in the way we did\r\n",
    "# for SGD. I'm sure there is way, I'll have to look closer later though."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multiclass Classification\r\n",
    "\r\n",
    "Algorithms that natively handle multiple classes:\r\n",
    "- SGD\r\n",
    "- Random Forest\r\n",
    "- Naive Bayes\r\n",
    "\r\n",
    "Algorithms that only handle binary classification:\r\n",
    "- Logistic Regression\r\n",
    "- SVM\r\n",
    "\r\n",
    "Strategies for turning a binary classifier into multiclass:\r\n",
    "1. One Versus One (OvO)\r\n",
    "- Binary classify between every pair of classes (0s and 1s, 0s and 2s, 1s and 2s, etc)\r\n",
    "- This means training 45 classifiers for MNIST, and it means running all 45 at prediction time\r\n",
    "- The winner of most duels is the class (for an actual 5, the 5 classifier should win 9 duels while all the rest should win 4 or 5 based on randomness)\r\n",
    "2. One Versus All (OvA)\r\n",
    "- Train one binary classifier for each class (0-detector, 1-detector, etc.) and judge by the classifier with the highest score. (For an actual 5, the 1-detector will likely give a low score, but the 5 and 6 might give high scores. Hopefully the 5-detector will give the highest score.)\r\n",
    "\r\n",
    "Algorithms like SVM that scale poorly to large training sets are actually more efficient to run with OvO (you only train on 2/10 of the training data for each classifier). But for all others OvR is more efficient\r\n",
    "\r\n",
    "Scikit learn automatically adjusts binary classifiers to work with multiclass data. See next cell for doing multiclass with SVM:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "# Training the SVM takes 3 minutes, so be default loading this from a saved model\r\n",
    "# svm_clf = SVC()\r\n",
    "# svm_clf.fit(X_train, y_train)\r\n",
    "# joblib.dump(svm_clf, \"svm_clf.pkl\")\r\n",
    "\r\n",
    "svm_clf = joblib.load(\"svm_clf.pkl\")\r\n",
    "svm_clf.predict([some_digit])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "some_digit_scores = svm_clf.decision_function([some_digit])\r\n",
    "some_digit_scores\r\n",
    "\r\n",
    "# Looking at the scores, you can see that class 5 did have the highest score with 9.31, but it\r\n",
    "# also socred 3 and 2 fairly highly as well."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Forcing sklearn to use OvA instead of OvO\r\n",
    "from sklearn.multiclass import OneVsRestClassifier\r\n",
    "\r\n",
    "# ovr_clf = OneVsRestClassifier(SVC())\r\n",
    "# ovr_clf.fit(X_train, y_train)\r\n",
    "joblib.dump(ovr_clf, \"ovr_clf.pkl\")\r\n",
    "ovr_clf = joblib.load(\"ovr_clf.pkl\")\r\n",
    "\r\n",
    "# Takes 20 minutes to run this cell"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(ovr_clf.predict([some_digit]))\r\n",
    "len(ovr_clf.estimators_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sgd_clf.fit(X_train, y_train)\r\n",
    "print(sgd_clf.predict([some_digit]))\r\n",
    "sgd_clf.decision_function([some_digit])\r\n",
    "\r\n",
    "# Takes 2 minutes to run this cell"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Improvement of SGD by applying standard deviation scaling to the pixel values\r\n",
    "# (See chapter 2)\r\n",
    "\r\n",
    "original_cv_scores = cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\r\n",
    "\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "scaler = StandardScaler()\r\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\r\n",
    "scaling_cv_scores = cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\r\n",
    "\r\n",
    "print(f\"Without std dev scaling: {original_cv_scores}\")\r\n",
    "print(f\"With std dev scaling: {scaling_cv_scores}\")\r\n",
    "\r\n",
    "# Takes 13 minutes to run this cell\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\r\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\r\n",
    "conf_mx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('.env')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "daf462d589485e3a7e4bbc9e21f8ffde85206cfcc2914ef5c7e82d9a8993ac7b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}